MultilayerPerceptrons
==
![question](https://raw.githubusercontent.com/timmy00274672/MultilayerPerceptrons/master/img/question.jpg)

- Based on X1, train two 2-layer feedforward neural network(FNN) with two 
and four nodes in the hidden layer. All hidden layer nodes use the 
hyperbolic tangent(tanh) as the activation function, while the output node 
uses the linear activation function. Run the standard backpropagation (BP)
algorithm for 9000 iterations with a learning rate of 0.01. Computing the 
training and test errors (based on X1 and X2, respectively) and plot the 
training points as well as the decision regions formed by each network. 
Also plot the training error versus the number of iterations.
- Repeat (I) by employing the adaptive BP algorithm for 6000 iterations, with 
a learning rate of 0.0001 and ri=1.05, rd=0.7, and c=1.04.